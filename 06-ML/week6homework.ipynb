{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Using scikit-learn for Naive Bayes and Logistic Regression\n",
    "\n",
    "Things we have built painfully from scratch can be done easily and quickly using the ```scikit-learn``` module.\n",
    "\n",
    "It can do the word-counting for us, transforming a column of tweets into a term-document matrix.\n",
    "\n",
    "Then it can run classification, using a variety of algorithms.\n",
    "\n",
    "```Scikit-learn``` can even do cross-validation automatically! Our lives are about to get a lot easier. \n",
    "\n",
    "(Of course, that means our goals can also become more ambitious.)\n",
    "\n",
    "Let's start by reading in our handy dataset of Trump tweets. Then I'll simplify the dataframe to reduce the number of columns. I'll also create a \"dummy\" numeric column to represent the \"class\" each tweet belongs to, using the technique illustrated by Kevin Markham's linear regression notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/tunder/Dropbox/courses/2017datasci/06-ML\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>isandroid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My economic policy speech will be carried live...</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join me in Fayetteville, North Carolina tomorr...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#ICYMI: \"Will Media Apologize to Trump?\" https...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Morell, the lightweight former Acting ...</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The media is going crazy. They totally distort...</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   source  isandroid\n",
       "0  My economic policy speech will be carried live...  android          1\n",
       "1  Join me in Fayetteville, North Carolina tomorr...   iphone          0\n",
       "2  #ICYMI: \"Will Media Apologize to Trump?\" https...   iphone          0\n",
       "3  Michael Morell, the lightweight former Acting ...  android          1\n",
       "4  The media is going crazy. They totally distort...  android          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, math\n",
    "import pandas as pd\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory: ' + cwd + '\\n')\n",
    "      \n",
    "relativepath = os.path.join('..', 'data', 'weekfour', 'trump.csv')\n",
    "trump = pd.read_csv(relativepath)\n",
    "\n",
    "def trump_test(a_data_frame, rowidx):\n",
    "    ''' Just a function that translates complex statusSource strings\n",
    "    into a simpler class label.\n",
    "    '''\n",
    "    \n",
    "    if 'iphone' in a_data_frame['statusSource'][rowidx]:\n",
    "        return 'iphone'\n",
    "    elif 'android' in a_data_frame['statusSource'][rowidx]:\n",
    "        return 'android'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Let's create a simple data frame with just two columns,\n",
    "# the tweet text and the source.\n",
    "\n",
    "tweet_text = trump['text']\n",
    "source = []\n",
    "for idx in trump.index:\n",
    "    source.append(trump_test(trump, idx))\n",
    "\n",
    "# We created source as a list. To add it to a data frame, we need\n",
    "# to make it a Series, which is the Pandas way of thinking about\n",
    "# an indexed list.\n",
    "source = pd.Series(source, index = trump.index)\n",
    "\n",
    "# Now let's create the Trump data frame.\n",
    "tdf = pd.concat([tweet_text, source], axis = 1)\n",
    "tdf.columns = ['text', 'source']\n",
    "\n",
    "# Let's filter it to get rid of any rows that aren't iphone\n",
    "# or android\n",
    "tdf = tdf[(tdf['source'] == 'android') | (tdf['source'] == 'iphone')]\n",
    "\n",
    "# Now to create a dummy numeric column. We'll need this later.\n",
    "tdf['isandroid'] = tdf.source.map({'iphone':0, 'android':1})\n",
    "\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn for word counting.\n",
    "\n",
    "Classifiers are going to demand two arguments: \n",
    "\n",
    "A) a matrix where rows are observations and columns are variables.\n",
    "B) a column of class labels for the observations.\n",
    "\n",
    "For text classification, (A) is a document-term matrix. Fortunately ```scikit-learn``` has a ```CountVectorizer()``` function that can produce this automatically.\n",
    "\n",
    "For full documentation, see:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': 100,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer(max_features = 100)\n",
    "\n",
    "# The max_features argument tells the vectorizer to return\n",
    "# a matrix with columns for only the p most common words.\n",
    "\n",
    "# Because this matrix could get quite large on a collection\n",
    "# of long documents, the vectorizer returns it in a special\n",
    "# 'sparse' format. But we're going to convert that to an ordinary\n",
    "# DataFrame for ease of inspection and manipulation.\n",
    "\n",
    "sparse_matrix = countvec.fit_transform(tdf['text'])\n",
    "termdoc = pd.DataFrame(sparse_matrix.toarray(), columns=countvec.get_feature_names())\n",
    "termdoc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799902661722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.86428571,  0.8       ,  0.79856115,  0.77697842,  0.76978417,\n",
       "        0.8057554 ,  0.81294964,  0.84172662,  0.81884058,  0.71014493])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "mnb = MultinomialNB()\n",
    "scores = cross_val_score(mnb, termdoc.as_matrix(), tdf['isandroid'], cv=10)\n",
    "print(sum(scores) / len(scores))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression\n",
    "\n",
    "I won't fully explain logistic regression here. I hope I have time to sketch a little on the whiteboard. But very briefly, it's a way of *mapping* linear regression onto a finite space between 0 and 1. This allows a regression algorithm to do the work of classification. It is sometimes more accurate than naive Bayes. (More importantly, it produces well-*calibrated* probabilistic predictions. When logistic regression tells you an instance has a 60% chance of being in class X, that number is actually meaningful. Naive Bayes, on the other hand, has a tendency to exaggerate, seeing instances as all the way toward one class or the other.) \n",
    "\n",
    "*Regularizing* the regression basically means adding a degree of deliberate fuzz to prevent overfitting. This is done by penalizing large weights in the model (so your decision boundary can't become all volatile and wiggly.)\n",
    "\n",
    "The upshot of all this is that regularized logistic regression gives you a parameter to tune: the regularization constant, ```C```, controlling the degree of fuzziness. In the scikit-learn implementation, small values of ```C``` mean strong (fuzzy) regularization.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827251664507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.88571429,  0.82142857,  0.79856115,  0.82014388,  0.77697842,\n",
       "        0.84892086,  0.86330935,  0.8705036 ,  0.81884058,  0.76811594])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C = .1)\n",
    "scores = cross_val_score(lr, termdoc.as_matrix(), tdf['isandroid'], cv=10)\n",
    "print(sum(scores) / len(scores))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework exercise 1: grid search\n",
    "\n",
    "Write a function that runs logistic regression repeatedly, in order to find the optimal value of C (regularization constant) for a given term-doc matrix and column of class labels. Use ten-fold cross-validation, as above.\n",
    "\n",
    "For this problem, it makes sense to search the space between 30 and .0003. Try using this list of possible settings:\n",
    "[30, 10, 3, 1, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, .0003]\n",
    "\n",
    "Your function should return two values: the optimal parameter for C, and the accuracy achieved at the optimal parameter.\n",
    "\n",
    "This is called \"grid search\" because you could, theoretically, optimize multiple parameters at once, with nested loops. Here, however, we're just optimizing one -- so it's more like \"line search.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework exercise 2: battle of the algorithms\n",
    "\n",
    "Which works better: logistic regression or naive Bayes?\n",
    "Does it matter how many features we use?\n",
    "\n",
    "Write a loop that generates term-doc matrices for the Trump dataset with different numbers of features, from 200 to 4000, increasing by 200 each time\n",
    "\n",
    "```for i in range(200, 4200, 200):```\n",
    "\n",
    "For each term-doc matrix, calculate the accuracy of naive Bayes, and also the accuracy of logistic regression (using your optimizer function from exercise 1 to get the best accuracy). Save both accuracies in a pair of growing lists.\n",
    "\n",
    "When the loop is complete, plot the accuracy of both classifiers. The x axis should be ```number of features,``` the y axis should be ```accuracy```, and you should have two lines: one for naive Bayes, and one for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Homework exercise 3: precision and recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = .1)\n",
    "termdoc.as_matrix(), tdf['isandroid']\n",
    "termdoc.as_matrix(), tdf['isandroid']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
