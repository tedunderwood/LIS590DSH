{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3: Functions and Hypothesis-Testing\n",
    "\n",
    "This homework begins by reviewing some things we accomplished in class, and defining some functions. Then it asks you to apply those functions to the Bechdel dataset. We won't fully replicate the data analysis performed in [the Bechdel article,](https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/) and spelled out (using R) in [this online analysis.](https://mran.microsoft.com/web/packages/fivethirtyeight/vignettes/bechdel.html) But we're going to build some elements of the process from the ground up, so that we genuinely understand what's going on â€” especially where hypothesis testing is involved.\n",
    "\n",
    "I should say up front that the approach we're taking to hypothesis-testing here is only one of several possible approaches; it's based on what statisticians call a *frequentist* theory of statistics. *Bayesian* statistics provide an alternative way of reasoning about these questions; we'll glance at that in a couple of weeks.\n",
    "\n",
    "To start with, let's test the significance of a correlation. We saw in class that movies that fail to pass the Bechdel test have lower median return-on-investment than those that do pass. This hints that movie studios may be controlled by a kind of sexism that is not only unjust, and not only not profitable, but actually self-destructive and money-*losing.* \n",
    "\n",
    "On the other hand, I mentioned in class that there's a potential *confounding variable* in this argument. Movies with lower budgets generally have higher ROI. So perhaps movies that pass the Bechdel test have higher ROI simply *because* they have lower budgets? We won't completely resolve that question below, but we'll build up to it. To begin with, is the relationship between budget and return-on-investment actually meaningful, or would we be likely to see relations of that kind emerge by chance?\n",
    "\n",
    "To test that, we'll start by reconstructing the functions we need for a correlation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our version:  0.766723820184\n",
      "Official version:  (0.76672382018354224, 0.015931317092922143)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We start (above) by importing the usual suspects.\n",
    "\n",
    "# Read the functions below to get a sense of how you can build up a complex\n",
    "# function by breaking it into elements.\n",
    "\n",
    "def dot(vectorA, vectorB):\n",
    "    ''' Calculates the dot product of two vectors; in other words, the\n",
    "    sum of the pairwise products of A and B.\n",
    "    '''\n",
    "    assert len(vectorA) == len(vectorB)\n",
    "    dotproduct = np.sum(vectorA * vectorB)\n",
    "    # that's the simpler version of a dot product, using\n",
    "    # numpy's automatic elementwise multiplication of vectors\n",
    "    \n",
    "    return dotproduct\n",
    "\n",
    "def covariance(vecA, vecB):\n",
    "    ''' Calculates the covariance of two vectors; in other words, the\n",
    "    tendency for A to be above its average when B is also above average,\n",
    "    and below when B is also below.\n",
    "    '''\n",
    "    \n",
    "    n = len(vecA)\n",
    "    if n > 1:\n",
    "        diffA = vecA - np.mean(vecA)\n",
    "        diffB = vecB - np.mean(vecB)\n",
    "        cov = dot(diffA, diffB) / n\n",
    "        \n",
    "        # In class we were using n-1 here, which gives sample covariance.\n",
    "        # The official function uses n (population covariance), so that's\n",
    "        # what I've substituted. It won't make much difference as we scale up\n",
    "        # to large n.\n",
    "        \n",
    "        return cov\n",
    "    else:\n",
    "        print('Error! covariance has no meaning if vectors have less than two elements.')\n",
    "        return float('NaN')\n",
    "\n",
    "def pearson_correlation(vecA, vecB):\n",
    "    ''' Calculates the Pearson correlation coefficient, r, for two\n",
    "    vectors.\n",
    "    '''\n",
    "    \n",
    "    cov = covariance(vecA, vecB)\n",
    "    \n",
    "    if np.std(vecA) == 0 or np.std(vecB) == 0:\n",
    "        print('Error! statistic undefined if standard deviations are zero.')\n",
    "        return float('NaN')\n",
    "    else:\n",
    "        pearson = cov / (np.std(vecA) * np.std(vecB))\n",
    "        return pearson\n",
    "\n",
    "    \n",
    "vectorA = np.array([1, 1, 3, 4, 5, 2, 7, 8, 7])\n",
    "vectorB = np.array([0, 2, 4, 5, 2, 6, 8, 9, 6])\n",
    "\n",
    "print('Our version: ', pearson_correlation(vectorA, vectorB))\n",
    "\n",
    "# Let's test that we're getting this roughly right\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print('Official version: ', pearsonr(vectorA, vectorB))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of what it means to calculate a p-value\n",
    "\n",
    "It appears that we're calculating Pearson's r the same way the scipy module does it. But they provide an additional piece of information: the p-value, or the probability that you would get an equally extreme r statistic if the null hypothesis were true -- the null hypothesis in this case being that there is no relation between the vectors. Let's test that in a simple, intuitive way.\n",
    "\n",
    "We'll just randomize the data and see how often we do get an equally large (or small) r value. Here's how you can randomize a list (or a numpy vector / pandas Series):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "vectorC = [1, 2, 3, 4, 5]\n",
    "random.shuffle(vectorC)\n",
    "print(vectorC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to repeatedly test correlation between our two vectors while randomizing one or both of them. The number of random correlations that are equal to or greater than our r value will tell us how often this could happen by chance.\n",
    "\n",
    "A Pearson's correlation coefficient can range from -1 (perfect inverse correlation) to 1 (perfect positive correlation). We'll use the *absolute value* of all the r values (i.e, forcing them all to be positive), because we didn't actually start with a hypothesis about whether this correlation was negative or positive. So *any* strong correlation (say an inverse correlation of -0.77) should count as evidence that \"a statistic this extreme could have occurred by chance.\" To test absolute magnitude rather than the raw (signed) r statistic, we'll transform everything using the ```abs()``` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.76672382018354246, 0.021)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def permutation_test_of_pearson_2tailed(vectorA, vectorB):\n",
    "    r = pearson_correlation(vectorA, vectorB)\n",
    "    absolute_r = abs(r)\n",
    "    \n",
    "    copiedA = np.array(vectorA)\n",
    "    copiedB = np.array(vectorB)\n",
    "    # we do that to avoid permanently shuffling vectorA\n",
    "    \n",
    "    random_r_values = list()\n",
    "    \n",
    "    number_of_tests = 1000\n",
    "    \n",
    "    for i in range(number_of_tests):\n",
    "        random.shuffle(copiedA)\n",
    "        random.shuffle(copiedB)\n",
    "        random_r = pearson_correlation(copiedA, copiedB)\n",
    "        random_r_values.append(abs(random_r))\n",
    " \n",
    "    # how many random r values are greater than or equal to the\n",
    "    # one we found in the actual vectors?\n",
    "    # to find out, we'll use the enumerate function,\n",
    "    # which returns elements of a list paired with\n",
    "    # their numeric index in the list\n",
    "    \n",
    "    random_r_values.sort(reverse = True)\n",
    "    for index, random_val in enumerate(random_r_values):\n",
    "        if random_val < absolute_r:\n",
    "            break\n",
    "    p = index / number_of_tests\n",
    "    \n",
    "    return r, p\n",
    "              \n",
    "permutation_test_of_pearson_2tailed(vectorA, vectorB)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p value calulated by our function won't be precisely equal to the value calculated by pearsonr, because there's an element of randomness here. But they will usually be in the same ballpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Editing the permutation test.\n",
    "\n",
    "Suppose we had started by predicting that the correlation between these two vectors would be *positive.* Then, in principle, we would only want to know how often a raw correlation coefficient greater than the observed value could occur by chance. Extreme negative correlations would not do anything to weaken our confidence in the significance of the pattern. This is what's called a one-tailed test.\n",
    "\n",
    "Copy the code of the 2-tailed permutation test above, paste it in the cell below, and edit it to make it 1-tailed. In other words, now you only want to ask how often the random Pearson's correlation is actually *larger* than the observed value, not how often it's *more extreme* than the observed value.\n",
    "\n",
    "If this is opaque, you might want to read [this account of the difference between one- and two-tailed tests.](https://en.m.wikipedia.org/wiki/One-_and_two-tailed_tests)\n",
    "\n",
    "The edit required is actually very simple; I just want to give you an occasion to read through the function and understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for problem 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Significance of correlation between budget and ROI\n",
    "\n",
    "Read in the Bechdel dataset (using the code below). Note that this code filters the dataset in several ways: for instance, it drops all rows where NaNs occur. I've done this for you using the .dropna() method. In this case, we've only dropped rows where NaNs occur in columns we're going to use (that's the significance of the \"subset\" argument).\n",
    "\n",
    "I also drop all films before 1990, in order to replicate the analysis at https://mran.microsoft.com/web/packages/fivethirtyeight/vignettes/bechdel.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/tunder/Dropbox/courses/2017datasci/03-Pandas\n",
      "\n",
      "Original shape:  (1794, 15)\n",
      "After dropping rows with NaN:  (1776, 15)\n",
      "After dropping rows before 1990:  (1600, 15)\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print('Current working directory: ' + cwd + '\\n')\n",
    "      \n",
    "relativepath = os.path.join('..', 'data', 'fivethirtyeight', 'bechdel.csv')\n",
    "bechdel = pd.read_csv(relativepath)\n",
    "print(\"Original shape: \", bechdel.shape)\n",
    "bechdel = bechdel.dropna(subset = ['domgross_2013', 'budget_2013', 'intgross_2013'])\n",
    "print(\"After dropping rows with NaN: \", bechdel.shape)\n",
    "bechdel = bechdel[bechdel['year'] > 1989]\n",
    "print(\"After dropping rows before 1990: \", bechdel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's figure out whether it's really meaningfully true that films with bigger budgets have lower return-on-investment. \n",
    "\n",
    "To figure this out, you'll need to start by creating a return-on-investment column. For our purposes let's focus on *domestic* ROI, which we'll calculate as the domestic gross receipts (inflation-corrected to 2013 dollars) divided by the film's budget (also inflation-corrected to 2013 dollars).\n",
    "\n",
    "Let's also adopt the conventional definition of statistical significance as p < 0.05, or roughly \"if the null hypothesis were true, a test statistic equally extreme could have occurred by chance more than 5% of the time.\"\n",
    "\n",
    "Given that definition, find out whether domestic return on investment has a significant (positive or negative) correlation with budget. Use 2013 dollars in both cases. In principle, you could test significance of the correlation using *either* the official ```pearsonr``` function or our ```permutation_test_of_pearson_2tailed``` function. But use both, to see how well they agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Difference between medians\n",
    "\n",
    "Let's also test the significance of the difference we observed between median ROI for films that do or don't pass the Bechdel test. \n",
    "\n",
    "First, calculate the median domestic return on investment for films that do, or don't, pass the Bechdel test (using the ```binary``` column to divide them).\n",
    "\n",
    "Then import the function ```median_test``` from ```scipy.stats``` (the same place we got pearsonr), and use it to assess the significance of the difference between medians. \n",
    "\n",
    "For a [full explanation of that function,](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.median_test.html) see the scipy documentation. You'll need to read this in order to figure out, for instance, how many arguments to send to the function.\n",
    "\n",
    "Basically, this function is doing something analogous to our permutation-test for significance of correlations. It's pooling all the data (both vectors of ROI figures, for films that pass the Bechdel test and films that fail) and calculating a \"grand median\" for the whole group. Then it asks how often we would randomly draw two groups of films from that master set that are as differently-distributed relative to the \"grand median\" as the two groups we actually passed in as arguments.\n",
    "\n",
    "It may be a useful reminder that when you index a DataFrame with single label like ```bechdel['budget_2013']```, you're selecting a column. But when you index using a list or vector of Boolean true/false values ```bechdel[bechdel['binary'] == 'PASS']```, you're selecting rows where that condition is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Discussion and conclusions.\n",
    "\n",
    "We haven't yet answered the question we started out with. To work it out completely in this assignment would require a few tricks I haven't yet taught you using Pandas, but let's leap forward a bit by relying on [the published analysis of this data I mentioned before.](https://mran.microsoft.com/web/packages/fivethirtyeight/vignettes/bechdel.html)\n",
    "\n",
    "This will be a little tricky to read, because they use R rather than Python/Pandas. But the principles are the same, and the significance of p-values, for instance, should now be legible to you, even if the format is a little different.\n",
    "\n",
    "How much can we infer from this evidence about sexism in the movie industry?\n",
    "Do you notice any important data analysis tricks used in the published analysis that we haven't covered yet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Write your conclusions in this Markdown cell. A paragraph of 5-8 sentences should be fine.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
